{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7193a0",
   "metadata": {
    "papermill": {
     "duration": 0.034817,
     "end_time": "2024-09-10T18:50:25.913835",
     "exception": false,
     "start_time": "2024-09-10T18:50:25.879018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "        word-wrap: break-word;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div style=\"display:flex; justify-content:space-around; align-items:center; background-color:#cccccc; padding:5px; border:2px solid #333333;\">\n",
    "    <a href=\"https://estudios.upct.es/grado/5251/inicio\" target=\"_blank\">\n",
    "    <img src=\"https://www.upct.es/contenido/universidad/galeria/identidad-2021/logos/logos-upct/marca-upct/marca-principal/horizontal/azul.png\" alt=\"UPCT\" style=\"height:145px; width:auto;\">\n",
    "    <a href=\"https://www.um.es/web/estudios/grados/ciencia-ingenieria-datos/\" target=\"_blank\">\n",
    "    <img src=\"https://www.um.es/documents/1073494/42130150/LogosimboloUMU-positivo.png\" alt=\"UMU\" style=\"height:200px; width:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30c905",
   "metadata": {
    "papermill": {
     "duration": 0.006835,
     "end_time": "2024-09-10T18:50:25.928004",
     "exception": false,
     "start_time": "2024-09-10T18:50:25.921169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Asignatura: **Deep Learning**\n",
    "\n",
    "## Titulación: **Grado en Ciencia e Ingeniería de Datos**\n",
    "\n",
    "## Práctica 1: Introducción al Deep Learning\n",
    "### **Sesión 1/3: Implementación de modelos de aprendizaje automático**\n",
    "\n",
    "**Autores**: Juan Morales Sánchez, Antonio Martínez Sánchez, José Luís Sancho Gómez y Juan Antonio Botía Blaya\n",
    "\n",
    "___\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "- Familiarización con TensorFlow y Python para *Deep Learning*\n",
    "- Manipulación de datos con tensores\n",
    "- Diseño y configuración de modelos supervisados\n",
    "- Entrenamiento y evaluación de modelos\n",
    "\n",
    "### Contenidos\n",
    "- [Entorno de trabajo](#entorno)\n",
    "- [Marcos para aprendizaje profundo](#librerias)\n",
    "- [Aprendizaje supervisado](#aprendizaje)\n",
    "- [Tensores y manipulación de datos](#tensores)\n",
    "- [La arquitectura de red](#arquitectura)\n",
    "- [Entrenamiento del modelo](#entrenamiento)\n",
    "- [Inferencia o predicciones del modelo](#inferencia)\n",
    "- [Ejercicios](#ejercicios)\n",
    "\n",
    "### Bibliografía\n",
    "- [Deep Learning with Python (segunda edición)](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
    "- [Dive into Deep Learning](https://d2l.ai/)\n",
    "\n",
    "### Requisitos \n",
    "<a class='anchor' id='requisitos'></a>\n",
    "\n",
    "- [Numpy](https://pypi.org/project/numpy/) (computación numérica)\n",
    "- [Scipy](https://pypi.org/project/scipy/) (computación científica)\n",
    "- [Scikit-learn](https://pypi.org/project/scikit-learn/) (*Machine Learning*)\n",
    "- [Scikit-image](https://pypi.org/project/scikit-image/) (*Image Processing*)\n",
    "- [Matplotlib](https://pypi.org/project/matplotlib/) y [Seaborn](https://pypi.org/project/seaborn/) (visualización de datos)\n",
    "- [Tensorflow](https://www.tensorflow.org/) 2.x que incluye a [Keras](https://www.tensorflow.org/guide/keras) 2.x (*Deep Learning*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849ee22",
   "metadata": {},
   "source": [
    "<div style=\"page-break-before: always;\"></div>\n",
    "\n",
    "<a class='anchor' id='entorno'></a>\n",
    "\n",
    "## Entorno de trabajo\n",
    "\n",
    "Se trabajará con notebooks de [Jupyter](https://jupyter.org/install) con código Python empleando como intérprete la última versión de [Miniconda](https://docs.anaconda.com/miniconda/). A continuación se enumeran los pasos a seguir para configurar un entorno virtual de Python adecuado. \n",
    "\n",
    "### Microconda + Visual Studio Code\n",
    "\n",
    "Para instalar miniconda:\n",
    "1. Accede al [repositorio de miniconda](https://repo.anaconda.com/miniconda/).\n",
    "2. Descarga e instala la última version (*latest*) disponible para tu sistema operativo.\n",
    "\n",
    "Para instalar Visual Studio Code:\n",
    "1. Descarga e instala VS Code desde [su sitio web oficial](https://code.visualstudio.com/download).\n",
    "2. Instala las extensiones de Python y Jupyter desde VS Code.\n",
    "\n",
    "### Configuración del entorno virtual\n",
    "1. Abre un nuevo terminal en tu sistema operativo (Linux, Windows o macOS).\n",
    "\n",
    "2. OPCIÓN 1: Crea un nuevo entorno virtual e instala paquetes desde repositorio de ``conda``:\n",
    "   ```bash\n",
    "   conda create --name dl --channel defaults python=3.9\n",
    "   conda activate dl\n",
    "   conda install numpy scipy scikit-learn scikit-image matplotlib seaborn mrcfile ipykernel tensorflow\n",
    "   ```\n",
    "\n",
    "2. OPCIÓN 2: Crea un nuevo entorno virtual e instala paquetes desde repositorio de ``pip`` (preferible si su equipo dispone de una GPU Nvidia):\n",
    "   ```bash\n",
    "   conda create --name dl --channel defaults python=3.9 pip\n",
    "   conda activate dl\n",
    "   pip install numpy scipy scikit-learn scikit-image matplotlib seaborn mrcfile ipykernel tensorflow[and-cuda]\n",
    "   ```\n",
    "\n",
    "3. Para gestionar el entorno, puedes desactivarlo y opcionalmente eliminarlo:\n",
    "   ```bash \n",
    "   conda deactivate\n",
    "   conda env remove --name dl --all\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "*Nota: Tenga en cuenta que [TensorFlow 2.10](https://www.tensorflow.org/install/pip#windows-native) es la última versión de TensorFlow que admite GPU en Windows nativo. A partir de [TensorFlow 2.11](https://www.tensorflow.org/install/pip#windows-wsl2) para utilizar GPU en Windows es necesario instalar TensorFlow sobre WSL2.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5f098",
   "metadata": {},
   "source": [
    "<a class='anchor' id='librerias'></a>\n",
    "\n",
    "## Marcos para aprendizaje profundo\n",
    "\n",
    "En la práctica las librerías de código abierto para la construcción y entrenamiento de modelos de aprendizaje profundo resultan ideales para implementar redes neuronales y procesar datos complejos como imágenes y texto.\n",
    "\n",
    "A continuación se resumen en una tabla comparativa las características de las 3 principales iniciativas o marcos de cómputo para *Deep Learning* que cumplen estos requisitos, y que actualmente concentran más de 90% del desarrollo en este campo: [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/) y [JAX](https://github.com/google/jax).\n",
    "\n",
    "| Aspecto            | [JAX](https://github.com/google/jax)                                                                                                                                         | [TensorFlow](https://www.tensorflow.org/)                                                                                                                                              | [PyTorch](https://pytorch.org/)                                                                                                                                          |\n",
    "|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Origen**         | Desarrollado por [Google Research](https://research.google/) (inspirado en [Autograd](https://github.com/HIPS/autograd))                                                                                           | Creado y mantenido por [Google Brain](https://ai.google/research/teams/brain/) (incluye componentes como [Keras](https://keras.io), [TFLite](https://www.tensorflow.org/lite), [TensorFlow.js](https://www.tensorflow.org/js), etc.) | Desarrollado por [Meta AI](https://ai.facebook.com/) (anteriormente Facebook), con apoyo de la comunidad de [GitHub](https://github.com/pytorch/pytorch)                                                       |\n",
    "| **Paradigma**      | Estilo funcional: usa transformaciones como [`jit`](https://jax.readthedocs.io/en/latest/jax.html#jax.jit), [`grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad), [`vmap`](https://jax.readthedocs.io/en/latest/jax.html#jax.vmap), etc., sobre sintaxis parecida a [NumPy](https://numpy.org/). | Ejecución imperativa por defecto ([Eager Execution](https://www.tensorflow.org/guide/eager)) y optimizaciones basadas en grafos internos (p. ej., [XLA](https://openxla.org/)).                                 | Ejecución imperativa (dinámica), muy similar a la programación en Python nativo.                                                                                                                               |\n",
    "| **Curva de aprendizaje**   | Moderada: sintaxis de [NumPy](https://numpy.org/) es familiar, pero el paradigma funcional (p. ej., `jit`, `vmap`) puede requerir un cambio de mentalidad.                                                            | Bastante accesible gracias a la integración con [Keras](https://keras.io). En casos avanzados, se puede profundizar en niveles de API más bajos (por ej., `tf.*` a bajo nivel).                                               | Normalmente considerada sencilla, útil para prototipado rápido y depuración, gracias a la ejecución imperativa paso a paso.                                                                                   |\n",
    "| **Rendimiento**    | Muy alto en [GPU](https://developer.nvidia.com/cuda-toolkit) y [TPU](https://cloud.google.com/tpu) gracias a la integración con [XLA](https://openxla.org/).                                                                                            | Excelente rendimiento en GPU y TPU; TensorFlow se integra con [XLA](https://openxla.org/) y ofrece muchas optimizaciones para despliegue en producción ([TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving), etc.). | Muy buen rendimiento en GPU ([CUDA](https://developer.nvidia.com/cuda-toolkit)). Soporte limitado para TPU (existen iniciativas de la comunidad, pero no un soporte oficial completo).                          |\n",
    "| **Ventajas**       | - Control granular sobre transformaciones y compilación<br>- Sintaxis similar a [NumPy](https://numpy.org/)<br>- Integración potente en [TPU](https://cloud.google.com/tpu).                                       | - Amplio ecosistema ([Keras](https://keras.io), [TFLite](https://www.tensorflow.org/lite), [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving), [TensorFlow.js](https://www.tensorflow.org/js), etc.)<br>- Respaldo empresarial<br>- Facilidad de despliegue | - Flujo imperativo muy intuitivo<br>- Amplia adopción en investigación (especialmente en visión, NLP, etc.)<br>- Gran cantidad de ejemplos y repositorios de la comunidad                                      |\n",
    "| **Desventajas**    | - Ecosistema más pequeño que PyTorch/TensorFlow<br>- El enfoque funcional no es tan común y puede ser menos intuitivo para principiantes                                                                            | - Puede ser abrumador por la gran cantidad de documentación y APIs<br>- A veces requiere atención a la versión y a la configuración de entornos (GPU/TPU)                                                                       | - No siempre tan optimizado como TensorFlow en escenarios de producción a gran escala<br>- Soporte oficial para TPU inexistente o muy limitado                                                                 |\n",
    "| **Comunidad y recursos**      | En crecimiento, con proyectos recientes en [GitHub](https://github.com/google/jax) y foros especializados, aunque menor que las de TensorFlow/PyTorch.                                                               | Muy grande, con abundantes recursos de aprendizaje (foros, cursos, documentación oficial, etc.).                                                                                       | Muy activa y enfocada en investigación; gran cantidad de *repos* y tutoriales en [GitHub](https://github.com/pytorch).                                                                                       |\n",
    "\n",
    "\n",
    "En conclusión, se puede afirmar que [PyTorch](https://pytorch.org/) y [TensorFlow](https://www.tensorflow.org/) son hoy por hoy los marcos más utilizadas para *Deep Learning* por diferentes motivos, mientras que [JAX](https://github.com/google/jax) se está abriendo paso especialmente en investigación y proyectos que busquen optimizaciones avanzadas mediante XLA.\n",
    "\n",
    "En las prácticas de esta asignatura nos utilizaremos [TensorFlow](https://www.tensorflow.org/), al considerar que el interfaz de integrado de [Keras](https://keras.io) ofrece la alternativa más accesible para un primer contacto con la implmentación de modelos de aprendizaje automático. Y a largo plazo también parece ofrecer una visión más amplia y completa de posibilidades y usos dentro del ámbito académico y profesional. La experiencia adquirida debería permitir una fácil adaptación a otros entornos como [PyTorch](https://pytorch.org/), e incluso una migración e código bastante directa.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505de2b",
   "metadata": {},
   "source": [
    "<a class='anchor' id='aprendizaje'></a>\n",
    "\n",
    "## Aprendizaje supervisado\n",
    "\n",
    "El **aprendizaje supervisado** constituye una categoría del aprendizaje automático en la cual se enseña a un modelo a realizar predicciones o clasificaciones basándose en un conjunto de datos etiquetados. Este tipo de aprendizaje se denomina \"supervisado\" porque el proceso de entrenamiento está guiado por una supervisión explícita a través de etiquetas que indican las respuestas correctas.\n",
    "\n",
    "### Tipos de Aprendizaje Supervisado\n",
    "1. **Regresión**: <a class='anchor' id='regresion'></a>\n",
    "   - El objetivo es predecir valores continuos.\n",
    "   - Ejemplo: Predecir el precio de una vivienda en función de sus características.\n",
    "\n",
    "2. **Clasificación**: <a class='anchor' id='clasificacion'></a>\n",
    "   - El objetivo es asignar categorías a las entradas.\n",
    "   - Ejemplo: Clasificar correos electrónicos como \"spam\" o \"no spam\".\n",
    "\n",
    "### Componentes del aprendizaje supervisado\n",
    "1. **Datos de entrenamiento**: <a class='anchor' id='datos'></a>\n",
    "   - Conjunto de ejemplos que incluyen:\n",
    "     - **Entradas** \\(X\\): Características o variables independientes.\n",
    "     - **Etiquetas** \\(Y\\): Salidas esperadas o variables dependientes (valores conocidos).\n",
    "\n",
    "2. **Modelo**: <a class='anchor' id='modelo'></a>\n",
    "   - Una función  f\\(X\\) que el sistema entrena para aproximar la relación entre X y Y -> # E[y | x]\n",
    "\n",
    "3. **Coste o pérdida (error)**: <a class='anchor' id='coste'></a>\n",
    "   - Una métrica que evalúa lo bien que las predicciones del modelo coinciden con las etiquetas reales.\n",
    "\n",
    "4. **Optimización**: <a class='anchor' id='optimizacion'></a>\n",
    "   - Proceso de ajuste de los parámetros del modelo para minimizar la pérdida mediante algoritmos como el descenso estocástico de gradiente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41543d",
   "metadata": {},
   "source": [
    "<a class='anchor' id='tensores'></a>\n",
    "\n",
    "## Tensores y manipulación de datos\n",
    "\n",
    "Las operaciones **tensores** son fundamentales en el aprendizaje profundo porque representan la estructura de datos básica que se utiliza para modelar y procesar información en redes neuronales. \n",
    "\n",
    "El concepto de **tensor** es una generalización de otros conceptos, que es aplicable a cualquier número de dimensiones:\n",
    "- **Escalar**: Tensor de 0 dimensiones (un número, como \\(5\\)).\n",
    "- **Vector**: Tensor de 1 dimensión (lista de números, como \\([1, 2, 3]\\)).\n",
    "- **Matriz**: Tensor de 2 dimensiones (tabla de números, o una imagen).\n",
    "- **Tensor de N dimensiones**: Datos estructurados en más de 2 dimensiones, como un cubo de datos o una secuencia de imágenes.\n",
    "\n",
    "Los tensores nos ofrecen una representación flexible de datos, pudiendo contener:\n",
    "- **Imágenes**: Representadas como tensores 3D (altura, ancho, canales).\n",
    "- **Texto**: Secuencias de palabras o caracteres codificadas como tensores 2D o 3D.\n",
    "- **Videos**: Tensores 4D (frames(tiempo), altura, ancho, canales(RGB)).\n",
    "\n",
    "Los parámetros del modelo (pesos, sesgos y otros parámetros aprendibles) también se representan como tensores. Durante el entrenamiento, los tensores se actualizan para minimizar la función de pérdida.\n",
    "\n",
    "La librería de [Numpy](https://pypi.org/project/numpy/) de computación numérica constituye una herramienta que habitualmente se emplea en una primera fase para la carga o acondicionamiento de datos numéricos almacenados en estructuras regulares (*arrays*), que podemos diferenciar según su dimensionalidad o rango.\n",
    "\n",
    "Cargaremos como ejemplo el clásico conjunto de datos [MNIST](https://es.wikipedia.org/wiki/Base_de_datos_MNIST), para analizar los atributos clave que se deben tener en cuenta en los datos y sus etiquetas asociadas para el aprendizaje supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112d2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cargando el conjunto de datos MNIST\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846e414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce53e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(train_images[0, :, :])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434b581",
   "metadata": {},
   "source": [
    "### Atributos clave de un conjunto de datos: rango, dimensiones y tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1272a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango del tensor del conjunto de entrenamiento:\n",
      " 3\n",
      "Dimensiones del conjunto de entrenamiento [samples x height x width]:\n",
      " (60000, 28, 28)\n",
      "Número de etiquetas del conjunto de entrenamiento:\n",
      " 60000\n",
      "Tipo del conjunto de entrenamiento:\n",
      " <class 'numpy.ndarray'> uint8\n",
      "Tipo de las etiquetas de entrenamiento:\n",
      " <class 'numpy.ndarray'> uint8\n",
      "Rango dinámico de valores del conjunto de entrenamiento:\n",
      " [np.uint8(0), np.uint8(255)]\n",
      "Valores de las etiquetas de entrenamiento:\n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      "Rango del tensor del conjunto de test:\n",
      " 3\n",
      "Dimensiones del conjunto de test [samples x height x width]:\n",
      " (10000, 28, 28)\n",
      "Número de etiquetas del conjunto de test:\n",
      " 10000\n",
      "Tipo del conjunto de test:\n",
      " <class 'numpy.ndarray'> uint8\n",
      "Tipo de las etiquetas de test:\n",
      " <class 'numpy.ndarray'> uint8\n",
      "Rango dinánico de valores del conjunto de entrenamiento:\n",
      " [np.uint8(0), np.uint8(255)]\n",
      "Valores de las etiquetas de entrenamiento:\n",
      " [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Rango del tensor del conjunto de entrenamiento:\\n\", train_images.ndim)\n",
    "print(\"Dimensiones del conjunto de entrenamiento [samples x height x width]:\\n\", train_images.shape)\n",
    "print(\"Número de etiquetas del conjunto de entrenamiento:\\n\", len(train_labels))\n",
    "print(\"Tipo del conjunto de entrenamiento:\\n\", type(train_images), train_images.dtype)\n",
    "print(\"Tipo de las etiquetas de entrenamiento:\\n\", type(train_labels), train_labels.dtype)\n",
    "print(\"Rango dinámico de valores del conjunto de entrenamiento:\\n\", [np.min(train_images), np.max(train_images)])\n",
    "print(\"Valores de las etiquetas de entrenamiento:\\n\", np.unique(train_labels))\n",
    "\n",
    "print(\"Rango del tensor del conjunto de test:\\n\", test_images.ndim)\n",
    "print(\"Dimensiones del conjunto de test [samples x height x width]:\\n\", test_images.shape)\n",
    "print(\"Número de etiquetas del conjunto de test:\\n\", len(test_labels))\n",
    "print(\"Tipo del conjunto de test:\\n\", type(test_images), test_images.dtype)\n",
    "print(\"Tipo de las etiquetas de test:\\n\", type(test_labels), test_labels.dtype)\n",
    "print(\"Rango dinánico de valores del conjunto de entrenamiento:\\n\", [np.min(test_images), np.max(test_images)])\n",
    "print(\"Valores de las etiquetas de entrenamiento:\\n\", np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a933ea",
   "metadata": {},
   "source": [
    "### Visualización de datos y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4302eaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Visualizando el quinta muestra del conjunto de imágenes y su etiqueta asociada\n",
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[4]\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.title(f\"Etiqueta asociada -> {train_labels[4]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1c64c",
   "metadata": {},
   "source": [
    "### Acondicionaniento de datos\n",
    "\n",
    "El **acondicionamiento de datos** es una etapa fundamental en el flujo de trabajo del aprendizaje automático y redes neuronales en general. Consiste en preparar los datos para que sean compatibles con los modelos y para mejorar el rendimiento y estabilidad del entrenamiento. Esta preparación incluye varias operaciones que ajustan los datos a los formatos, rangos o estructuras requeridas. Entre las operaciones típicas que podemos englobar dentro de esta categoría se encuentran habitualmente: \n",
    "\n",
    "1. **Normalización de datos**:\n",
    "   - Escalar los valores de las características a un rango común, como [0, 1] ó [-1, 1].\n",
    "   - Ejemplo: Dividir los valores de píxel de imágenes (en el rango 0-255) por 255 para normalizarlos entre [0, 1].\n",
    "   - Beneficio:\n",
    "     - Mejora la estabilidad del modelo.\n",
    "     - Acelera la convergencia durante el entrenamiento.\n",
    "\n",
    "2. **Aplanamiento de dimensiones**:\n",
    "   - Transformar datos multidimensionales en vectores unidimensionales.\n",
    "   - Ejemplo: Imágenes 2D (28x28 píxeles) a un vector de 784 elementos para alimentarlas en capas densas.\n",
    "   - Beneficio:\n",
    "     - Permite la entrada de datos a modelos que requieren estructuras lineales.\n",
    "\n",
    "3. **Conversión de tipo de datos**:\n",
    "   - Cambiar el tipo de los datos o las etiquetas, como de enteros (`int`, `uint`) a coma flotante (`float`).\n",
    "   - Ejemplo: Convertir etiquetas categóricas a `float32` para cálculos numéricos.\n",
    "   - Beneficio:\n",
    "     - Asegura compatibilidad con el diseño del modelo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06125d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango del tensor del conjunto de entrenamiento:\n",
      " 2\n",
      "Dimensiones del conjunto de entrenamiento [samples x height x width]:\n",
      " (60000, 784)\n",
      "Número de etiquetas del conjunto de entrenamiento:\n",
      " 60000\n",
      "Tipo del conjunto de entrenamiento:\n",
      " <class 'numpy.ndarray'> float32\n",
      "Tipo de las etiquetas de entrenamiento:\n",
      " <class 'numpy.ndarray'> uint8\n",
      "Rango dinánico de valores del conjunto de entrenamiento:\n",
      " [np.float32(0.0), np.float32(1.0)]\n",
      "Valores de las etiquetas de entrenamiento:\n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      "Rango del tensor del conjunto de test:\n",
      " 2\n",
      "Dimensiones del conjunto de test [samples x height x width]:\n",
      " (10000, 784)\n",
      "Número de etiquetas del conjunto de test:\n",
      " 10000\n",
      "Tipo del conjunto de test:\n",
      " <class 'numpy.ndarray'> float32\n",
      "Tipo de las etiquetas de test:\n",
      " <class 'numpy.ndarray'> uint8\n",
      "Rango dinánico de valores del conjunto de entrenamiento:\n",
      " [np.float32(0.0), np.float32(1.0)]\n",
      "Valores de las etiquetas de entrenamiento:\n",
      " [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))  # Aplanamiento\n",
    "train_images = train_images.astype(\"float32\") / 255  # Normalización + tipo flotante\n",
    "test_images = test_images.reshape((10000, 28 * 28))  # Aplanamiento\n",
    "test_images = test_images.astype(\"float32\") / 255  # Normalización + tipo flotante\n",
    "\n",
    "print(\"Rango del tensor del conjunto de entrenamiento:\\n\", train_images.ndim)\n",
    "print(\"Dimensiones del conjunto de entrenamiento [samples x height x width]:\\n\", train_images.shape)\n",
    "print(\"Número de etiquetas del conjunto de entrenamiento:\\n\", len(train_labels))\n",
    "print(\"Tipo del conjunto de entrenamiento:\\n\", type(train_images), train_images.dtype)\n",
    "print(\"Tipo de las etiquetas de entrenamiento:\\n\", type(train_labels), train_labels.dtype)\n",
    "print(\"Rango dinánico de valores del conjunto de entrenamiento:\\n\", [np.min(train_images), np.max(train_images)])\n",
    "print(\"Valores de las etiquetas de entrenamiento:\\n\", np.unique(train_labels))\n",
    "\n",
    "print(\"Rango del tensor del conjunto de test:\\n\", test_images.ndim)\n",
    "print(\"Dimensiones del conjunto de test [samples x height x width]:\\n\", test_images.shape)\n",
    "print(\"Número de etiquetas del conjunto de test:\\n\", len(test_labels))\n",
    "print(\"Tipo del conjunto de test:\\n\", type(test_images), test_images.dtype)\n",
    "print(\"Tipo de las etiquetas de test:\\n\", type(test_labels), test_labels.dtype)\n",
    "print(\"Rango dinánico de valores del conjunto de entrenamiento:\\n\", [np.min(test_images), np.max(test_images)])\n",
    "print(\"Valores de las etiquetas de entrenamiento:\\n\", np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f24ca0",
   "metadata": {},
   "source": [
    "### Noción de lote de datos\n",
    "\n",
    "Recordemos en primer lugar que una **época** es un ciclo completo en el que el modelo procesa todo el conjunto de datos de entrenamiento una vez. Durante una época, el modelo utiliza todos los ejemplos del conjunto de datos para calcular los gradientes y ajustar los pesos. Por otra parte, y a lo largo de cada época, un **lote de datos** (*batch*) es un subconjunto del conjunto de datos de entrenamiento que el modelo procesa en cada iteración del entrenamiento.\n",
    "\n",
    "Por tanto, en lugar de procesar todos los datos de entrenamiento de una vez en cada época, se dividen en pequeños grupos (lotes), y el modelo ajusta sus parámetros utilizando únicamente los datos del lote actual (*Mini-Batch Training*). Algunas de las ventajas del uso de lotes de datos son:\n",
    "\n",
    "1. **Eficiencia computacional**:\n",
    "   - Procesar datos en lotes aprovecha la computación paralela de GPUs/TPUs.\n",
    "2. **Mejor estimación del gradiente**:\n",
    "   - Mini-batches ofrecen un compromiso entre ruido y precisión en el cálculo del gradiente.\n",
    "3. **Eficiencia del uso de memoria**:\n",
    "   - Los lotes permiten entrenar con conjuntos de datos grandes sin necesidad de cargar todo el conjunto en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa5d2548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del lote de datos [samples x length]:\n",
      " (128, 784)\n",
      "Dimensiones del lote de etiquetas [samples]:\n",
      " 128\n"
     ]
    }
   ],
   "source": [
    "n = 3 # lote número 3\n",
    "batch_size = 128 # tamaño del lote\n",
    "batch = train_images[batch_size * n:batch_size * (n + 1)]\n",
    "label_batch = train_labels[batch_size * n:batch_size * (n + 1)]\n",
    "print(\"Dimensiones del lote de datos [samples x length]:\\n\", batch.shape)\n",
    "print(\"Dimensiones del lote de etiquetas [samples]:\\n\", len(label_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3686ef",
   "metadata": {},
   "source": [
    "<a class='anchor' id='arquitectura'></a>\n",
    "\n",
    "## La arquitectura de red\n",
    "\n",
    "La **arquitectura de red** refleja la estructura y diseño de una red neuronal, incluyendo las capas, cómo están conectadas, los tipos de operaciones que realizan y los hiperparámetros asociados. Es una especificación que define cómo se organiza la red para procesar datos y alcanzar predicciones.\n",
    "\n",
    "La arquitectura influye directamente en la capacidad del modelo para aprender patrones de los datos. Elegir la arquitectura correcta es el primer requisito para un buen rendimiento del modelo y su capacidad de generalización. Algunos componentes claves de la arquitectura de red son:\n",
    "\n",
    "1. **Capas**:\n",
    "   - Agrupación de operaciones de un determinado tipo, que transforman los datos desde la entrada hasta la salida.\n",
    "   - Ejemplo:\n",
    "     - Capas densas (*fully connected*).\n",
    "     - Capas convolucionales (*convolutional layers*).\n",
    "     - Capas recurrentes (*recurrent layers*).\n",
    "\n",
    "2. **Neuronas por capa**:\n",
    "   - El número de neuronas determina la capacidad de cada capa para reconocer patrones. El número de capas y la cantidad de neuronas de cada capa determina el número total de parámetros o pesos entrenables del modelo.\n",
    "\n",
    "3. **Funciones de activación**:\n",
    "   - Introducen no linealidades para que la red pueda aprender relaciones complejas.\n",
    "   - Ejemplo: `ReLU`, `sigmoid`, `tanh`.\n",
    "\n",
    "4. **Conexiones entre capas**:\n",
    "   - Puede fluir solo hacía delante (*feedforward*) o incluir retroalimentación (como en redes recurrentes).\n",
    "\n",
    "5. **Hiperparámetros**:\n",
    "   - Aspectos configurables de la red, como el número de capas, tamaño del lote, tasa de aprendizaje, etc.\n",
    "\n",
    "\n",
    "Existen varias formas de definir la arquitectura de un modelo de aprendizaje automático en TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5442f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\miniconda3\\envs\\dl_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definición secuencial (opción 1)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, Model\n",
    "model = Sequential([\n",
    "    layers.Dense(512, activation=\"relu\", input_shape=(28 * 28, )), # capa oculta\n",
    "    layers.Dense(10, activation=\"softmax\") # capa de salida\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "# Definición secuencial (opción 2)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(512, activation=\"relu\", input_shape=(28 * 28, ))) # capa oculta\n",
    "model.add(layers.Dense(10, activation=\"softmax\")) # capa de salida\n",
    "model.summary()\n",
    "\n",
    "# Definición funcional\n",
    "input_layer = layers.Input(shape=(28 * 28, ))  # Capa de entrada\n",
    "hidden_layer = layers.Dense(512, activation=\"relu\")(input_layer)  # Capa oculta\n",
    "output_layer = layers.Dense(10, activation=\"softmax\")(hidden_layer)  # Capa de salida\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a96a7",
   "metadata": {},
   "source": [
    "Observa que con una definición ``Sequential`` del modelo la ``InputLayer`` no aparece en el resumen, incluso si se incluye explícitamente. En ``Functional``, siempre aparece porque la entrada es explícita como parte del diseño del modelo. Esto sucede porque en el flujo típico de ``Sequential`` la capa de entrada se infiere automáticamente a partir de los datos proporcionados en la primera capa, y no se considera como una capa separada en el grafo de computación, si no como parte del flujo de datos implícito, lo cual es más simple y resulta óptimo para arquitecturas convencionales, en particular aquellas que tienen una sola entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291fc93",
   "metadata": {},
   "source": [
    "### Configurar el modelo para el aprendizaje\n",
    "\n",
    "La configuración y compilación del modelo conlleva definir las siguientes componentes claves:\n",
    "\n",
    "   - **Función de coste o pérdida**:\n",
    "     - Establece una métrica para valorar lo bien/mal que está prediciendo el modelo.\n",
    "     - Ejemplo: `mean squared error` para regresión, `categorical crossentropy` para clasificación.\n",
    "   - **Optimizador**:\n",
    "     - Algoritmo que ajusta los parámetros del modelo para minimizar la pérdida.\n",
    "     - Ejemplo: `Adam`, `SGD`.\n",
    "   - **Métricas adicionales**:\n",
    "     - Evalúan las prestaciones del modelo durante el entrenamiento y la validación.\n",
    "     - Ejemplo: precisión (`accuracy`), error absoluto medio (`MAE`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "986412bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',                       # Optimizador\n",
    "    loss='sparse_categorical_crossentropy',    # Función de pérdida\n",
    "    metrics=['accuracy']                       # Métrica\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d55d48",
   "metadata": {},
   "source": [
    "<a class='anchor' id='entrenamiento'></a>\n",
    "\n",
    "## Entrenamiento del modelo\n",
    "\n",
    "El método `fit` de TensorFlow es una función fundamental en la API de Keras que entrena un modelo de aprendizaje profundo utilizando un conjunto de datos. Este método ejecuta el proceso de entrenamiento, integrando todas las fases necesarias: cálculo de pérdidas, optimización, y evaluación de métricas.\n",
    "\n",
    "Al llamar al método `fit`, TensorFlow sigue estos pasos:\n",
    "\n",
    "1. **Preparación del entrenamiento**:\n",
    "   - Verifica que el modelo esté correctamente compilado con:\n",
    "     - Una función de pérdida (`loss`).\n",
    "     - Un optimizador (`optimizer`).\n",
    "     - Opcionalmente, métricas (`metrics`) para evaluar las prestaciones.\n",
    "\n",
    "2. **Procesamiento de los datos**:\n",
    "   - Divide los datos en lotes según el tamaño definido en `batch_size`.\n",
    "   - Si se emplea validación, separa una parte de los datos para evaluación.\n",
    "\n",
    "3. **Bucle de entrenamiento**:\n",
    "   Para cada época:\n",
    "   - **Paso hacia adelante (*forward pass*)**:\n",
    "     - Calcula las predicciones del modelo para un lote de datos.\n",
    "   - **Cálculo de la pérdida**:\n",
    "     - Compara las predicciones con las etiquetas reales usando la función de pérdida.\n",
    "   - **Paso hacia atrás (*backward pass*)**:\n",
    "     - Calcula los gradientes de los parámetros del modelo con respecto a la pérdida.\n",
    "   - **Actualización de los pesos**:\n",
    "     - Ajusta los pesos del modelo utilizando el optimizador configurado.\n",
    "\n",
    "4. **Validación (opcional)**:\n",
    "   - Al final de cada época, evalúa el modelo en el conjunto de validación.\n",
    "\n",
    "5. **Almacenamiento de resultados**:\n",
    "   - Guarda métricas como la pérdida y otras definidas en `metrics` para cada época.\n",
    "\n",
    "El método `fit` devuelve un histórico (objeto `history`) que contiene un registro de las prestaciones del modelo durante el entrenamiento, mediante un diccionario con las métricas por época (`loss`, `mae`, `val_loss`, etc.). Además `fit` es compatible con `callbacks` para personalizar y extender el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ad398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8863 - loss: 0.3860\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9717 - loss: 0.0944\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0600\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0432\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9914 - loss: 0.0288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_images, train_labels, # datos y etiquetas de entrenamiento\n",
    "    epochs=5,               # Número de épocas\n",
    "    batch_size=64           # Tamaño del lote\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a35868",
   "metadata": {},
   "source": [
    "<a class='anchor' id='inferencia'></a>\n",
    "\n",
    "## Inferencia o predicciones del modelo\n",
    "\n",
    "El método `predict` de TensorFlow en la API de Keras se utiliza para realizar **inferencia** con un modelo ya entrenado. Es decir, genera predicciones a partir de nuevos datos de entrada, aplicando únicamente la fase de **propagación hacia adelante** (*forward pass*), sin actualizar los pesos del modelo, es decir realiza los siguientes pasos:\n",
    "\n",
    "1. **Propagación hacia adelante**:\n",
    "   - Toma los datos de entrada y los pasa a través de las capas del modelo.\n",
    "   - Realiza todas las operaciones definidas en el modelo, como multiplicaciones de tensores, funciones de activación y normalización.\n",
    "\n",
    "2. **Cálculo de las salidas**:\n",
    "   - Produce una salida en función de las configuraciones del modelo:\n",
    "     - Para problemas de regresión, devuelve valores continuos: el valor estimado por el modelo basado en los datos de entrada.\n",
    "     - Para problemas de clasificación, devuelve probabilidades de cada clase (si la última capa es una función `softmax`). Normalmente se elige la clase con mayor probabilidad para obtener las clases predichas (`np.argmax(y_pred, axis=-1)`).\n",
    "\n",
    "3. **Agrupación en lotes (opcional)**:\n",
    "   - Divide los datos en lotes si el conjunto de datos es grande, para optimizar el uso de memoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d20bac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Etiqueta: 7, Predicción: 7\n",
      "Etiqueta: 2, Predicción: 2\n",
      "Etiqueta: 1, Predicción: 1\n",
      "Etiqueta: 0, Predicción: 0\n",
      "Etiqueta: 4, Predicción: 4\n",
      "Etiqueta: 1, Predicción: 1\n",
      "Etiqueta: 4, Predicción: 4\n",
      "Etiqueta: 9, Predicción: 9\n",
      "Etiqueta: 5, Predicción: 6\n",
      "Etiqueta: 9, Predicción: 9\n",
      "Etiqueta: 0, Predicción: 0\n",
      "Etiqueta: 6, Predicción: 6\n",
      "Etiqueta: 9, Predicción: 9\n",
      "Etiqueta: 0, Predicción: 0\n",
      "Etiqueta: 1, Predicción: 1\n",
      "Etiqueta: 5, Predicción: 5\n",
      "Etiqueta: 9, Predicción: 9\n",
      "Etiqueta: 7, Predicción: 7\n",
      "Etiqueta: 3, Predicción: 3\n",
      "Etiqueta: 4, Predicción: 4\n"
     ]
    }
   ],
   "source": [
    "test_digits = test_images[0:20]\n",
    "predictions = model.predict(test_digits) # probabilidades de clase (one hot encoding)\n",
    "predictions = predictions.argmax(axis=-1) # clase de mayor probabilidad\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Etiqueta: {test_labels[i]}, Predicción: {pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961daa4",
   "metadata": {},
   "source": [
    "<a class='anchor' id='evaluacion'></a>\n",
    "\n",
    "### Evaluación del modelo\n",
    "\n",
    "La función `evaluate` de TensorFlow en la API de Keras evalúa el modelo entrenado en un conjunto de datos específico, calculando la pérdida y las métricas definidas durante la compilación del modelo. Es una forma de medir el rendimiento del modelo, ya sea en datos de validación o de test, sin modificar los parámetros del modelo. Conlleva la siguiente secuencia:\n",
    "\n",
    "1. **Propagación hacia adelante (*forward pass*)**:\n",
    "   - Pasa los datos de entrada a través del modelo para generar predicciones.\n",
    "\n",
    "2. **Cálculo de la pérdida**:\n",
    "   - Compara las predicciones con las etiquetas reales utilizando la función de pérdida definida.\n",
    "\n",
    "3. **Cálculo de métricas**:\n",
    "   - Evalúa las métricas configuradas al compilar el modelo, como precisión, MAE, etc.\n",
    "\n",
    "4. **Agrupación en lotes (opcional)**:\n",
    "   - Si el conjunto de datos es grande, divide los datos en lotes para optimizar el uso de memoria.\n",
    "\n",
    "5. **Promedio de resultados**:\n",
    "   - Calcula la pérdida y las métricas promedio en todos los lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c898419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - accuracy: 0.9763 - loss: 0.0746\n",
      "test_acc: 0.9801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48e003",
   "metadata": {},
   "source": [
    "<a class='anchor' id='ejercicios'></a>\n",
    "\n",
    "## Ejercicios\n",
    "\n",
    "**E1:** Crear un modelo de clasificación binaria para predecir si la suma de dos números es mayor a 1 usando un conjunto de datos sintético.\n",
    "\n",
    "   - Se generan 1000 muestras con dos características cada una (`x_data`).\n",
    "   - La etiqueta (`y_data`) es `1` si la suma de las dos características es mayor que 1, y `0` en caso contrario.\n",
    "      - Ejemplo:\n",
    "         - Entrada: `[0.6, 0.7]` → Suma: `1.3` → Etiqueta: `1`.\n",
    "         - Entrada: `[0.3, 0.2]` → Suma: `0.5` → Etiqueta: `0`.\n",
    "\n",
    "   - Se divide el conjunto en entrenamiento (80%) y prueba (20%).\n",
    "\n",
    "   - Construir una red neuronal con dos capas:\n",
    "     - Capa oculta: 64 neuronas y función de activación `relu`.\n",
    "     - Capa de salida: 1 neurona con activación `sigmoid` para obtener una probabilidad entre 0 y 1 (para clasificación binaria).\n",
    "\n",
    "   - Optimizador: `adam`, que ajusta los pesos durante el entrenamiento.\n",
    "   - Pérdida: `binary_crossentropy`, que es adecuada para problemas de clasificación binaria.\n",
    "   - Métrica: `accuracy`, que mide el porcentaje de predicciones correctas.\n",
    "\n",
    "   - Se entrena en el conjunto de entrenamiento durante 30 épocas con un tamaño de lote de 32.\n",
    "   - Utiliza el 20% de los datos de entrenamiento como conjunto de validación para monitorear el rendimiento.\n",
    "\n",
    "   - El modelo se evalúa en el conjunto de prueba, calculando la pérdida y la precisión.\n",
    "\n",
    "   - Se realizan predicciones en nuevas muestras (`x_sample`).\n",
    "   - La salida del modelo son probabilidades (valores entre 0 y 1), y se convierten en clases (`0` o `1`) comparando con un umbral de 0.5.\n",
    "      - Ejemplo:\n",
    "         - Entrada: `[0.6, 0.6]` → Probabilidad: `0.95` → Clase: `1`.\n",
    "\n",
    "**E2:** Cambiar el tamaño de la red (número de neuronas o capas) y analizar el impacto.\n",
    "\n",
    "**E3:** Probar diferentes funciones de activación y analizar el impacto.\n",
    "\n",
    "\n",
    "\n",
    "**E4:** Crear un modelo de regresión para predecir la suma de dos números usando un conjunto de datos sintético..\n",
    "\n",
    "   - Se generan números aleatorios para las entradas ``x_1`` y ``x_2``, separándolo un 30% de ellos para test.\n",
    "   - Las etiquetas ``y`` son la suma de ``x_1 + x_2``.\n",
    "\n",
    "   - Una red neuronal simple con:\n",
    "     - Una capa oculta de 64 neuronas con activación ReLU.\n",
    "     - Una capa de salida con 1 neurona (regresión).\n",
    "\n",
    "   - Optimización con Adam.\n",
    "   - Pérdida: `mse` (error cuadrático medio), adecuada para problemas de regresión.\n",
    "   - Métrica: `mae` (error absoluto medio), que mide la desviación promedio.\n",
    "\n",
    "   - Se entrena el modelo durante 20 épocas con un tamaño de lote de 32.\n",
    "   - El 20% de los datos se reserva para validación en el entrenamiento.\n",
    "\n",
    "   - El modelo se evalúa en el conjunto de test no visto durante el entrenamiento.\n",
    "\n",
    "   - Se realiza inferencia con nuevas muestras para verificar el rendimiento del modelo, por ejemplo para una entrada como `[0.3, 0.7]`, la salida debería estar cerca de `1.0`.\n",
    "\n",
    "**E5:** Cambiar el tamaño del conjunto de datos y analizar el impacto.\n",
    "\n",
    "**E6:** Aumentar la complejidad del problema (por ejemplo, usando ruido en las etiquetas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0ae51",
   "metadata": {},
   "source": [
    "## E1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c21dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\miniconda3\\envs\\dl_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5018 - loss: 0.6750 - val_accuracy: 0.5500 - val_loss: 0.6520\n",
      "Epoch 2/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5259 - loss: 0.6514 - val_accuracy: 0.6125 - val_loss: 0.6296\n",
      "Epoch 3/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5754 - loss: 0.6340 - val_accuracy: 0.6375 - val_loss: 0.6100\n",
      "Epoch 4/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6336 - loss: 0.6115 - val_accuracy: 0.6812 - val_loss: 0.5901\n",
      "Epoch 5/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6875 - loss: 0.5932 - val_accuracy: 0.7500 - val_loss: 0.5706\n",
      "Epoch 6/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7503 - loss: 0.5625 - val_accuracy: 0.7875 - val_loss: 0.5499\n",
      "Epoch 7/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8006 - loss: 0.5414 - val_accuracy: 0.8438 - val_loss: 0.5279\n",
      "Epoch 8/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8404 - loss: 0.5200 - val_accuracy: 0.8813 - val_loss: 0.5050\n",
      "Epoch 9/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8619 - loss: 0.4872 - val_accuracy: 0.8938 - val_loss: 0.4831\n",
      "Epoch 10/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8788 - loss: 0.4719 - val_accuracy: 0.9000 - val_loss: 0.4600\n",
      "Epoch 11/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8843 - loss: 0.4492 - val_accuracy: 0.9000 - val_loss: 0.4392\n",
      "Epoch 12/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9108 - loss: 0.4151 - val_accuracy: 0.9062 - val_loss: 0.4167\n",
      "Epoch 13/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9090 - loss: 0.4033 - val_accuracy: 0.9187 - val_loss: 0.3973\n",
      "Epoch 14/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9337 - loss: 0.3776 - val_accuracy: 0.9312 - val_loss: 0.3782\n",
      "Epoch 15/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9240 - loss: 0.3719 - val_accuracy: 0.9312 - val_loss: 0.3600\n",
      "Epoch 16/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9436 - loss: 0.3474 - val_accuracy: 0.9375 - val_loss: 0.3421\n",
      "Epoch 17/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9247 - loss: 0.3221 - val_accuracy: 0.9500 - val_loss: 0.3249\n",
      "Epoch 18/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9317 - loss: 0.3080 - val_accuracy: 0.9563 - val_loss: 0.3098\n",
      "Epoch 19/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9492 - loss: 0.2963 - val_accuracy: 0.9438 - val_loss: 0.2967\n",
      "Epoch 20/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9487 - loss: 0.2811 - val_accuracy: 0.9563 - val_loss: 0.2827\n",
      "Epoch 21/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9567 - loss: 0.2748 - val_accuracy: 0.9563 - val_loss: 0.2704\n",
      "Epoch 22/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9644 - loss: 0.2528 - val_accuracy: 0.9563 - val_loss: 0.2621\n",
      "Epoch 23/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9504 - loss: 0.2556 - val_accuracy: 0.9500 - val_loss: 0.2511\n",
      "Epoch 24/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9588 - loss: 0.2517 - val_accuracy: 0.9563 - val_loss: 0.2399\n",
      "Epoch 25/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9623 - loss: 0.2372 - val_accuracy: 0.9688 - val_loss: 0.2335\n",
      "Epoch 26/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9763 - loss: 0.2082 - val_accuracy: 0.9750 - val_loss: 0.2227\n",
      "Epoch 27/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9779 - loss: 0.2121 - val_accuracy: 0.9688 - val_loss: 0.2173\n",
      "Epoch 28/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9756 - loss: 0.2038 - val_accuracy: 0.9688 - val_loss: 0.2102\n",
      "Epoch 29/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.1936 - val_accuracy: 0.9750 - val_loss: 0.2037\n",
      "Epoch 30/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9819 - loss: 0.1933 - val_accuracy: 0.9937 - val_loss: 0.1962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x248efc559a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fijar la semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generación de datos sintéticos\n",
    "x_data = np.random.rand(1000, 2)  \n",
    "y_data = (x_data.sum(axis=1) > 1).astype(int)\n",
    "\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba (80/20) con estratificación\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "\n",
    "# Construir la arquitectura del modelo\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(2,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Preparar el modelo para el entrenamiento: Algoritmo de optimización, función de pérdida y métrica de evaluación\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02953e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9449 - loss: 0.2251 \n",
      "Pérdida en prueba: 0.2156, Precisión en prueba: 0.9500\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo con el conjunto de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Pérdida en prueba: {loss:.4f}, Precisión en prueba: {accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Realizamos predicciones con muestras no antes vistas(20 primeras):\n",
    "predictions = model.predict(X_test[:20])\n",
    "predicted_classes = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a06434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 1, Predicción: [0]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 0, Predicción: [0]\n",
      "Etiqueta: 0, Predicción: [0]\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(predicted_classes):\n",
    "    print(f\"Etiqueta: {y_test[i]}, Predicción: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442d243",
   "metadata": {},
   "source": [
    "## E2 y E3:\n",
    "\n",
    "**E2:** Cambiar el tamaño de la red (número de neuronas o capas) y analizar el impacto.\n",
    "\n",
    "Se puede observar que conforme incrementamos la cantidad de neuronas y el número de capas, sobre todo con lo segundo, a la red se le permite aprender asociaciones más complejos entre los datos lo que conlleva a conseguir unas métricas de exactitud muy buenas.\n",
    "\n",
    "**E3:** Probar diferentes funciones de activación y analizar el impacto.\n",
    "\n",
    "No observamos grandes impactos a la hora de usar una función de activación u otra, esto, es en parte, porque la red no es lo suficientemente profunda para que pueda variar el redimiento del modelo. Normalmente, la sigmoide y tanh tienen problemas con el desvanecimiento de gradiente cuando se aumenta el nº de capas ocultas. Surge la ReLU que soluciona este problema y permite el desarrollo de estos modelos realmente profundos.\n",
    "\n",
    "Con la ReLU surge otro problema, la posible aparición de neuronas muertas por la propia forma de la función, las neuronas cuyos pesos y bias asociados la llevan a la parte negativa de la función, se van a 0, y una vez allí no se pueden recuperar, por lo que esas neuronas paran su aprendizaje. Como solución surgen varias variantes de la ReLU como puede ser la Leaky ReLU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b140e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\miniconda3\\envs\\dl_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ruben\\miniconda3\\envs\\dl_env\\lib\\site-packages\\keras\\src\\ops\\nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5191 - loss: 0.6465 - val_accuracy: 0.5188 - val_loss: 0.5967\n",
      "Epoch 2/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4809 - loss: 0.5649 - val_accuracy: 0.5188 - val_loss: 0.4835\n",
      "Epoch 3/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4757 - loss: 0.4527 - val_accuracy: 0.5188 - val_loss: 0.3369\n",
      "Epoch 4/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4923 - loss: 0.2944 - val_accuracy: 0.5188 - val_loss: 0.2170\n",
      "Epoch 5/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4876 - loss: 0.2110 - val_accuracy: 0.5188 - val_loss: 0.1585\n",
      "Epoch 6/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5140 - loss: 0.1430 - val_accuracy: 0.5188 - val_loss: 0.1260\n",
      "Epoch 7/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5068 - loss: 0.1206 - val_accuracy: 0.5188 - val_loss: 0.1101\n",
      "Epoch 8/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5261 - loss: 0.1004 - val_accuracy: 0.5188 - val_loss: 0.0886\n",
      "Epoch 9/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4956 - loss: 0.0993 - val_accuracy: 0.5188 - val_loss: 0.1038\n",
      "Epoch 10/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4910 - loss: 0.0921 - val_accuracy: 0.5188 - val_loss: 0.0703\n",
      "Epoch 11/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4857 - loss: 0.0917 - val_accuracy: 0.5188 - val_loss: 0.0693\n",
      "Epoch 12/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5057 - loss: 0.0615 - val_accuracy: 0.5188 - val_loss: 0.0581\n",
      "Epoch 13/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4945 - loss: 0.0681 - val_accuracy: 0.5188 - val_loss: 0.0578\n",
      "Epoch 14/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4956 - loss: 0.0573 - val_accuracy: 0.5188 - val_loss: 0.0652\n",
      "Epoch 15/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4671 - loss: 0.0564 - val_accuracy: 0.5188 - val_loss: 0.0796\n",
      "Epoch 16/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4856 - loss: 0.0707 - val_accuracy: 0.5188 - val_loss: 0.0466\n",
      "Epoch 17/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5050 - loss: 0.0621 - val_accuracy: 0.5188 - val_loss: 0.0426\n",
      "Epoch 18/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5371 - loss: 0.0465 - val_accuracy: 0.5188 - val_loss: 0.0528\n",
      "Epoch 19/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5029 - loss: 0.0480 - val_accuracy: 0.5188 - val_loss: 0.0386\n",
      "Epoch 20/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5062 - loss: 0.0515 - val_accuracy: 0.5188 - val_loss: 0.0438\n",
      "Epoch 21/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5192 - loss: 0.0465 - val_accuracy: 0.5188 - val_loss: 0.0376\n",
      "Epoch 22/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5159 - loss: 0.0385 - val_accuracy: 0.5188 - val_loss: 0.0348\n",
      "Epoch 23/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4945 - loss: 0.0421 - val_accuracy: 0.5188 - val_loss: 0.0467\n",
      "Epoch 24/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4915 - loss: 0.0413 - val_accuracy: 0.5188 - val_loss: 0.0317\n",
      "Epoch 25/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5153 - loss: 0.0390 - val_accuracy: 0.5188 - val_loss: 0.0363\n",
      "Epoch 26/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4903 - loss: 0.0319 - val_accuracy: 0.5188 - val_loss: 0.0289\n",
      "Epoch 27/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5218 - loss: 0.0297 - val_accuracy: 0.5188 - val_loss: 0.0303\n",
      "Epoch 28/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4691 - loss: 0.0443 - val_accuracy: 0.5188 - val_loss: 0.0419\n",
      "Epoch 29/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5029 - loss: 0.0433 - val_accuracy: 0.5188 - val_loss: 0.0279\n",
      "Epoch 30/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5219 - loss: 0.0327 - val_accuracy: 0.5188 - val_loss: 0.0677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x248efe2d580>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fijar la semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generar 1000 muestras con 2 características cada una\n",
    "x_data = np.random.rand(1000, 2)  # Valores en el rango [0, 1]\n",
    "\n",
    "# Calcular etiquetas según la regla dada\n",
    "y_data = (x_data.sum(axis=1) > 1).astype(int)\n",
    "\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba (80/20) con estratificación\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "\n",
    "# Construir el modelo de red neuronal\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='tanh', input_shape=(2,)),\n",
    "    layers.Dense(128, activation='tanh'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3abe3bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5061 - loss: 0.0729  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\miniconda3\\envs\\dl_env\\lib\\site-packages\\keras\\src\\ops\\nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida en prueba: 0.0746, Precisión en prueba: 0.5050\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000248F7BCCB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 1, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n",
      "Etiqueta: 0, Predicción: [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\miniconda3\\envs\\dl_env\\lib\\site-packages\\keras\\src\\ops\\nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (20, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con el conjunto de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Pérdida en prueba: {loss:.4f}, Precisión en prueba: {accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Realizar predicciones con muestras no antes vistas(20 primeras):\n",
    "predictions = model.predict(X_test[:20])\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Mostrar las etiquetas reales y las predicciones\n",
    "for i, pred in enumerate(predicted_classes):\n",
    "    print(f\"Etiqueta: {y_test[i]}, Predicción: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1b22c",
   "metadata": {},
   "source": [
    "## E4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e47b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\miniconda3\\envs\\dl_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7818.5581 - mae: 75.6666 - val_loss: 2.4002 - val_mae: 1.3686\n",
      "Epoch 2/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7201 - mae: 1.0794 - val_loss: 0.7871 - val_mae: 0.7484\n",
      "Epoch 3/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7620 - mae: 0.7302 - val_loss: 0.6235 - val_mae: 0.6784\n",
      "Epoch 4/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6091 - mae: 0.6602 - val_loss: 0.5201 - val_mae: 0.6168\n",
      "Epoch 5/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5100 - mae: 0.6017 - val_loss: 0.4318 - val_mae: 0.5478\n",
      "Epoch 6/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4258 - mae: 0.5390 - val_loss: 0.3573 - val_mae: 0.4985\n",
      "Epoch 7/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3557 - mae: 0.4901 - val_loss: 0.2965 - val_mae: 0.4469\n",
      "Epoch 8/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3041 - mae: 0.4437 - val_loss: 0.2485 - val_mae: 0.4014\n",
      "Epoch 9/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2526 - mae: 0.4003 - val_loss: 0.2117 - val_mae: 0.3667\n",
      "Epoch 10/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2264 - mae: 0.3760 - val_loss: 0.1829 - val_mae: 0.3381\n",
      "Epoch 11/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1843 - mae: 0.3398 - val_loss: 0.1609 - val_mae: 0.3166\n",
      "Epoch 12/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1621 - mae: 0.3172 - val_loss: 0.1437 - val_mae: 0.2991\n",
      "Epoch 13/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1451 - mae: 0.2995 - val_loss: 0.1297 - val_mae: 0.2841\n",
      "Epoch 14/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1329 - mae: 0.2870 - val_loss: 0.1178 - val_mae: 0.2713\n",
      "Epoch 15/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1252 - mae: 0.2763 - val_loss: 0.1068 - val_mae: 0.2587\n",
      "Epoch 16/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1139 - mae: 0.2650 - val_loss: 0.0973 - val_mae: 0.2476\n",
      "Epoch 17/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1050 - mae: 0.2542 - val_loss: 0.0882 - val_mae: 0.2371\n",
      "Epoch 18/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0913 - mae: 0.2377 - val_loss: 0.0794 - val_mae: 0.2250\n",
      "Epoch 19/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0835 - mae: 0.2280 - val_loss: 0.0704 - val_mae: 0.2124\n",
      "Epoch 20/20\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0744 - mae: 0.2153 - val_loss: 0.0626 - val_mae: 0.2009\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0662 - mae: 0.2064\n",
      "Error absoluto medio en test: 0.20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Predicciones: [ 43.72282 156.40602  83.1649   84.58633 151.98137]\n",
      "Etiquetas reales: [ 43.43692  156.4885    83.273285  84.14958  151.90277 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "x1 = np.random.uniform(0, 100, 10000).astype('float32')  # Números aleatorios entre 0 y 100\n",
    "x2 = np.random.uniform(0, 100, 10000).astype('float32')  # Números aleatorios entre 0 y 100\n",
    "y = x1 + x2  # Etiqueta: la suma de x1 y x2\n",
    "\n",
    "# Convertir en matriz de características\n",
    "X = np.column_stack((x1, x2))\n",
    "\n",
    "# Dividir en conjunto de entrenamiento (70%) y test (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Construcción del modelo\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(2,)),  # Capa oculta\n",
    "    layers.Dense(1)  # Capa de salida para regresión\n",
    "])\n",
    "\n",
    "# 3. Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 4. Entrenar el modelo con validación\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 5. Evaluar el modelo en datos de test\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f'Error absoluto medio en test: {mae:.2f}')\n",
    "\n",
    "# 6. Inferencia con nuevos datos\n",
    "new_samples = X_test[:5] # Ejemplos de prueba\n",
    "predictions = model.predict(new_samples)\n",
    "print(\"Predicciones:\", predictions.T)\n",
    "print(\"Etiquetas reales:\", y_test[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d14c2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones: [[ 43.72282 156.40602  83.1649   84.58633 151.98137]]\n",
      "Etiquetas reales: [ 43.43692  156.4885    83.273285  84.14958  151.90277 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicciones:\", predictions.T)\n",
    "print(\"Etiquetas reales:\", y_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.503705,
   "end_time": "2024-09-10T18:50:39.408748",
   "environment_variables": {},
   "exception": null,
   "input_path": "./s02.ipynb",
   "output_path": "/home/rufernan/local/DOCENCIA/_2024_2025/C1_PI/PRACTICAS/s02/s02.ipynb",
   "parameters": {},
   "start_time": "2024-09-10T18:50:24.905043",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
