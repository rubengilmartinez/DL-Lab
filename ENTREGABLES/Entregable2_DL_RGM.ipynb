{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Entregable 2 Deep Learning: Redes Convolucionales*\n",
    "- Rubén Gil Martínez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **P2.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E1:** Carga las imágenes de entrada y sus identificadores de clase correspondientes. Almacénalos en dos tensores con las dimensiones adecuadas para servir de entrada y salida (entrenamiento) respectivamente de la red neuronal construida.\n",
    "\n",
    "**E2:** Aleatoriza la posición de las imágenes y consecuentemente los identificadores.\n",
    "\n",
    "**E3:** Particiona los tensores de entrada en dos, asigna el 80% de las imágenes para el entrenamiento y el 20% para la validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import os\n",
    "from keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: (400, 50, 50, 1) (400,)\n",
      "Tamaño del conjunto de validación: (100, 50, 50, 1) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Ruta del conjunto de datos\n",
    "data_path = '../data/imgs/SNR_high'\n",
    "\n",
    "# Dimensiones de las imágenes\n",
    "img_size = (50, 50)\n",
    "\n",
    "# Cargar imágenes y etiquetas\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        # Extraer la etiqueta del nombre del archivo\n",
    "        label = str(filename.split('_')[0])  \n",
    "        \n",
    "        # Cargar imagen en escala de grises\n",
    "        img_path = os.path.join(data_path, filename)\n",
    "        image = load_img(img_path, color_mode='grayscale', target_size=img_size)\n",
    "        image = img_to_array(image) / 255.0  # Normalizar los valores de los píxeles\n",
    "        \n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "# Convertir listas a arrays de NumPy\n",
    "images = np.array(images)  # Dimensión (N, 50, 50, 1)\n",
    "labels = np.array(labels)  # Dimensión (N,)\n",
    "\n",
    "# Mezclar aleatoriamente las imágenes y etiquetas\n",
    "indices = np.random.permutation(len(images))\n",
    "images = images[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Partición en entrenamiento (80%) y validación (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Imprimir dimensiones resultantes\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape, y_train.shape)\n",
    "print(\"Tamaño del conjunto de validación:\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E4:** Construye un MLP que realice la clasificación de las imágenes según se especifica en el apartado [MLP baseline](mlp). Utiliza el algoritmo de optimización y la función de pérdidas que creas más oportuna.\n",
    "\n",
    "**E5:** ¿Cuántos parámetros a entrenar tendría esta red?\n",
    "\n",
    "**E6:** Entrena el modelo y encuentra la combinación de tamaño de batch y épocas que genere mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E7:** Muestra la evolución de la función de pérdidas para los conjuntos de datos de entrenamiento y validación a lo largo del proceso de entrenamiento.\n",
    "\n",
    "**E8:** ¿Qué precisión tiene el modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E9:** Construye una red neuronal para clasificar las imágenes del dataset que contenga capas convolucionales y un MLP. Utiliza el algoritmo de optimización y la función de pérdidas que creas más oportuna.\n",
    "\n",
    "**E10:** ¿Cuántos parámetros a entrenar tendría esta red? Compáralos con los que se obtuvieron con el MLP.\n",
    "\n",
    "**E11:** Entrena el modelo y determina la precisión de la clasificación. Discute los resultados en comparación con los que se obtuvieron con el MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E12:** Muestra la matriz de confusión resultado de procesar el conjunto de datos de test.\n",
    "\n",
    "**E13:** ¿Qué dos clases presentan mayor confusión mutua? ¿Por qué? Visualiza unas cuantas imágenes con clasificaciones correctas e incorrectas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
